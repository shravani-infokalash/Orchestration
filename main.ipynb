{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ceaa95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"databricks_formula_executor\")\n",
    "\n",
    "# CREATE WIDGET FOR PAYLOAD\n",
    "dbutils.widgets.text(\"payload\", \"{}\", \"JSON Payload\")\n",
    "\n",
    "\n",
    "def parse_sql_query(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parse SQL query to understand structure and generate appropriate dummy data\"\"\"\n",
    "    query_lower = query.lower().strip()\n",
    "    \n",
    "    # Extract column names from SELECT statement\n",
    "    columns = []\n",
    "    if 'select' in query_lower:\n",
    "        # Simple regex to extract columns (basic implementation)\n",
    "        select_part = re.search(r'select\\s+(.*?)\\s+from', query_lower, re.DOTALL)\n",
    "        if select_part:\n",
    "            columns_str = select_part.group(1)\n",
    "            # Split by comma and clean up\n",
    "            raw_columns = [col.strip() for col in columns_str.split(',')]\n",
    "            for col in raw_columns:\n",
    "                if col == '*':\n",
    "                    # If SELECT *, generate some common columns\n",
    "                    columns = ['id', 'product_id', 'value', 'amount', 'quantity', 'date_created']\n",
    "                    break\n",
    "                else:\n",
    "                    # Extract column name (remove aliases, functions, etc.)\n",
    "                    clean_col = re.sub(r'\\s+as\\s+\\w+', '', col)\n",
    "                    clean_col = re.sub(r'[^\\w.]', '_', clean_col)\n",
    "                    columns.append(clean_col[:50])  # Limit column name length\n",
    "    \n",
    "    # If no columns found, use defaults\n",
    "    if not columns:\n",
    "        columns = ['id', 'result', 'value']\n",
    "    \n",
    "    # Determine number of rows based on query complexity\n",
    "    if any(keyword in query_lower for keyword in ['count', 'sum', 'avg', 'group by']):\n",
    "        row_count = random.randint(1, 10)  # Aggregation queries return fewer rows\n",
    "    elif 'limit' in query_lower:\n",
    "        # Try to extract limit number\n",
    "        limit_match = re.search(r'limit\\s+(\\d+)', query_lower)\n",
    "        if limit_match:\n",
    "            row_count = min(int(limit_match.group(1)), 1000)\n",
    "        else:\n",
    "            row_count = random.randint(10, 100)\n",
    "    else:\n",
    "        row_count = random.randint(50, 500)\n",
    "    \n",
    "    return {\n",
    "        'columns': columns,\n",
    "        'row_count': row_count\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_dummy_value(column_name: str, row_index: int) -> Any:\n",
    "    \"\"\"Generate appropriate dummy value based on column name patterns\"\"\"\n",
    "    column_lower = column_name.lower()\n",
    "    \n",
    "    # ID columns\n",
    "    if 'id' in column_lower:\n",
    "        return random.randint(1000, 99999)\n",
    "    \n",
    "    # Date columns\n",
    "    if any(date_keyword in column_lower for date_keyword in ['date', 'time', 'created', 'updated']):\n",
    "        base_date = datetime.now()\n",
    "        days_offset = random.randint(-365, 0)\n",
    "        result_date = datetime(base_date.year, base_date.month, base_date.day) \n",
    "        return (result_date.replace(day=1) if days_offset < -30 else result_date).isoformat()\n",
    "    \n",
    "    # Amount/Money columns\n",
    "    if any(money_keyword in column_lower for money_keyword in ['amount', 'price', 'cost', 'revenue', 'profit']):\n",
    "        return round(random.uniform(10.0, 10000.0), 2)\n",
    "    \n",
    "    # Quantity/Count columns\n",
    "    if any(qty_keyword in column_lower for qty_keyword in ['quantity', 'count', 'number', 'qty']):\n",
    "        return random.randint(1, 100)\n",
    "    \n",
    "    # Percentage columns\n",
    "    if any(pct_keyword in column_lower for pct_keyword in ['percent', 'rate', 'ratio']):\n",
    "        return round(random.uniform(0.0, 100.0), 2)\n",
    "    \n",
    "    # Status/Category columns\n",
    "    if any(status_keyword in column_lower for status_keyword in ['status', 'state', 'category', 'type']):\n",
    "        statuses = ['active', 'inactive', 'pending', 'completed', 'processing']\n",
    "        return random.choice(statuses)\n",
    "    \n",
    "    # Name columns\n",
    "    if 'name' in column_lower:\n",
    "        names = ['Product A', 'Product B', 'Service X', 'Item Y', 'Component Z']\n",
    "        return f\"{random.choice(names)} {row_index}\"\n",
    "    \n",
    "    # Boolean columns\n",
    "    if any(bool_keyword in column_lower for bool_keyword in ['is_', 'has_', 'active', 'enabled']):\n",
    "        return random.choice([True, False])\n",
    "    \n",
    "    # Default: numeric value\n",
    "    return round(random.uniform(1.0, 1000.0), 2)\n",
    "\n",
    "\n",
    "def execute_sql_comprehensive(query: str, debug: bool = False, client_id: str = \"\", product_id: str = None, task_key: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"Generate dummy SQL results instead of executing against SQL warehouse\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        logger.info(f\"Generating dummy data for query at {datetime.now().strftime('%H:%M:%S')} - Task: {task_key}, Product: {product_id}\")\n",
    "        \n",
    "        if debug:\n",
    "            logger.debug(f\"Query: {query}\")\n",
    "        \n",
    "        # Add random delay to simulate execution time\n",
    "        execution_delay = random.uniform(0.5, 3.0)\n",
    "        time.sleep(execution_delay)\n",
    "        \n",
    "        # Parse query to determine structure\n",
    "        query_info = parse_sql_query(query)\n",
    "        columns = query_info['columns']\n",
    "        row_count = query_info['row_count']\n",
    "        \n",
    "        # Generate dummy data\n",
    "        typed_rows = []\n",
    "        for i in range(row_count):\n",
    "            row = {}\n",
    "            for col in columns:\n",
    "                row[col] = generate_dummy_value(col, i + 1)\n",
    "            typed_rows.append(row)\n",
    "        \n",
    "        # Create column type information\n",
    "        column_types = []\n",
    "        for col in columns:\n",
    "            col_lower = col.lower()\n",
    "            if 'id' in col_lower:\n",
    "                column_types.append('BIGINT')\n",
    "            elif any(date_keyword in col_lower for date_keyword in ['date', 'time']):\n",
    "                column_types.append('TIMESTAMP')\n",
    "            elif any(bool_keyword in col_lower for bool_keyword in ['is_', 'has_', 'active', 'enabled']):\n",
    "                column_types.append('BOOLEAN')\n",
    "            else:\n",
    "                column_types.append('DOUBLE')\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        inline_data = {\n",
    "            \"rows\": typed_rows,\n",
    "            \"row_count\": len(typed_rows),\n",
    "            \"columns\": columns,\n",
    "            \"format\": \"JSON_ARRAY\"\n",
    "        }\n",
    "        \n",
    "        # Generate dummy external links\n",
    "        external_links = [\n",
    "            f\"https://test-storage.example.com/results/{task_key}_{product_id}_{int(time.time())}.csv\"\n",
    "        ]\n",
    "        \n",
    "        # Generate dummy manifest\n",
    "        manifest = {\n",
    "            \"format\": \"CSV\",\n",
    "            \"total_row_count\": row_count,\n",
    "            \"total_chunk_count\": 1,\n",
    "            \"chunks\": [{\n",
    "                \"chunk_index\": 0,\n",
    "                \"row_offset\": 0,\n",
    "                \"row_count\": row_count\n",
    "            }],\n",
    "            \"schema\": {\n",
    "                \"columns\": [\n",
    "                    {\"name\": col, \"type_text\": col_type, \"type_name\": col_type}\n",
    "                    for col, col_type in zip(columns, column_types)\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"SUCCEEDED\",\n",
    "            \"execution_time_seconds\": execution_time,\n",
    "            \"query\": query,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"inline\": inline_data,\n",
    "            \"external_links\": external_links,\n",
    "            \"manifest\": manifest,\n",
    "            \"format\": \"BOTH\",\n",
    "            \"row_count\": row_count,\n",
    "            \"error\": None,\n",
    "            \"product_id\": product_id,\n",
    "            \"task_key\": task_key,\n",
    "            \"dummy_data\": True  # Flag to indicate this is dummy data\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        logger.error(f\"Error generating dummy data for task {task_key}, product {product_id}: {str(e)}\")\n",
    "\n",
    "        return {\n",
    "            \"status\": \"FAILED\",\n",
    "            \"execution_time_seconds\": execution_time,\n",
    "            \"query\": query,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"results\": None,\n",
    "            \"error\": str(e),\n",
    "            \"product_id\": product_id,\n",
    "            \"task_key\": task_key,\n",
    "            \"dummy_data\": True\n",
    "        }\n",
    "\n",
    "\n",
    "def process_task(task: Dict[str, Any], workflow_run_id: str, debug: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"Process an individual task from the workflow\"\"\"\n",
    "    task_key = task.get(\"task_key\", \"\")\n",
    "    description = task.get(\"description\", \"\")\n",
    "    sql_task = task.get(\"sql_task\", {})\n",
    "    timeout_seconds = task.get(\"timeout_seconds\", 30)\n",
    "    \n",
    "    # Extract product information from SQL task parameters\n",
    "    sql_parameters = sql_task.get(\"parameters\", {})\n",
    "    product_id = sql_parameters.get(\"product_id\", \"\")\n",
    "    calculation_order = sql_parameters.get(\"calculation_order\", \"0\")\n",
    "    \n",
    "    # Extract SQL query\n",
    "    query_info = sql_task.get(\"query\", {})\n",
    "    sql_query = query_info.get(\"query\", \"\")\n",
    "    \n",
    "    logger.info(f\"Processing task: {task_key} - {description}\")\n",
    "    logger.info(f\"Product ID: {product_id}, Calculation Order: {calculation_order}\")\n",
    "    \n",
    "    task_result = {\n",
    "        \"task_key\": task_key,\n",
    "        \"product_id\": product_id,\n",
    "        \"description\": description,\n",
    "        \"calculation_order\": int(calculation_order) if calculation_order.isdigit() else 0,\n",
    "        \"timeout_seconds\": timeout_seconds,\n",
    "        \"status\": \"PENDING\",\n",
    "        \"dependencies\": [dep.get(\"task_key\") for dep in task.get(\"depends_on\", [])],\n",
    "        \"sql_result\": None,\n",
    "        \"error\": None,\n",
    "        \"start_time\": datetime.now().isoformat(),\n",
    "        \"end_time\": None\n",
    "    }\n",
    "    \n",
    "    # Execute the SQL query if it exists (now with dummy data)\n",
    "    if sql_query:\n",
    "        try:\n",
    "            # Determine client_id from workflow tags or default\n",
    "            client_id = workflow_run_id.split('_')[-1]  # Extract from workflow_run_id\n",
    "            \n",
    "            sql_result = execute_sql_comprehensive(\n",
    "                query=sql_query,\n",
    "                debug=debug,\n",
    "                client_id=client_id,\n",
    "                product_id=product_id,\n",
    "                task_key=task_key\n",
    "            )\n",
    "            \n",
    "            task_result[\"sql_result\"] = sql_result\n",
    "            task_result[\"status\"] = sql_result.get(\"status\", \"UNKNOWN\")\n",
    "            \n",
    "            if sql_result.get(\"status\") == \"FAILED\":\n",
    "                task_result[\"error\"] = sql_result.get(\"error\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing task {task_key}: {str(e)}\")\n",
    "            task_result[\"status\"] = \"FAILED\"\n",
    "            task_result[\"error\"] = str(e)\n",
    "    else:\n",
    "        # If no SQL query, mark as completed\n",
    "        task_result[\"status\"] = \"SUCCEEDED\"\n",
    "    \n",
    "    task_result[\"end_time\"] = datetime.now().isoformat()\n",
    "    return task_result\n",
    "\n",
    "\n",
    "def webhook_notify_individual_task(task_result: Dict[str, Any], workflow_info: Dict[str, Any], workflow_payload: Dict[str, Any]):\n",
    "    \"\"\"Send individual task result to webhook\"\"\"\n",
    "    webhook_url = \"https://dev.morpheo.ai/api/webhook/job-status\"  # Replace with actual webhook URL\n",
    "\n",
    "    task_key = task_result.get(\"task_key\")\n",
    "    product_id = task_result.get(\"product_id\")\n",
    "    sql_result = task_result.get(\"sql_result\", {})\n",
    "    \n",
    "    # Extract workflow information\n",
    "    workflow_run_id = workflow_info.get(\"workflow_run_id\", \"\")\n",
    "    client_id = workflow_info.get(\"client_id\", \"\")\n",
    "    \n",
    "    # Get email based on task status\n",
    "    email_notifications = workflow_payload.get(\"email_notifications\", {})\n",
    "    task_status = sql_result.get(\"status\", \"UNKNOWN\") if sql_result else \"NO_QUERY\"\n",
    "    \n",
    "    # Select appropriate email based on status\n",
    "    if task_status in [\"FAILED\", \"ERROR\"]:\n",
    "        emails = email_notifications.get(\"on_failure\", [\"\"])\n",
    "        logger.info(f\"Task {task_key} failed, using failure emails: {emails}\")\n",
    "    elif task_status == \"SUCCEEDED\":\n",
    "        emails = email_notifications.get(\"on_success\", [\"\"])\n",
    "        logger.info(f\"Task {task_key} succeeded, using success emails: {emails}\")\n",
    "    else:\n",
    "        # For unknown or pending status, use success email as default\n",
    "        emails = email_notifications.get(\"on_success\", [\"\"])\n",
    "        logger.info(f\"Task {task_key} has status {task_status}, using success emails as default: {emails}\")\n",
    "    \n",
    "    client_email = emails[0] if emails else \"\"\n",
    "    logger.info(f\"Selected email for task {task_key}: {client_email}\")\n",
    "    \n",
    "    # Prepare individual task payload\n",
    "    individual_payload = {\n",
    "        \"product_id\": product_id,\n",
    "        \"EstimatedTime\": sql_result.get(\"execution_time_seconds\", 0),\n",
    "        \"client_id\": client_id,\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"email\": client_email,\n",
    "        \"name\": task_result.get(\"description\", \"\"),\n",
    "        \"result\": {\n",
    "            \"execution_summary\": {\n",
    "                \"end_time\": task_result.get(\"end_time\"),\n",
    "                \"failed_queries\": 1 if sql_result.get(\"status\") == \"FAILED\" else 0,\n",
    "                \"start_time\": task_result.get(\"start_time\"),\n",
    "                \"successful_queries\": 1 if sql_result.get(\"status\") == \"SUCCEEDED\" else 0,\n",
    "                \"total_execution_time\": sql_result.get(\"execution_time_seconds\", 0),\n",
    "                \"total_queries\": 1\n",
    "            },\n",
    "            \"task\": {\n",
    "                \"task_key\": task_key,\n",
    "                \"product_id\": product_id,\n",
    "                \"description\": task_result.get(\"description\", \"\"),\n",
    "                \"calculation_order\": task_result.get(\"calculation_order\", 0),\n",
    "                \"timeout_seconds\": task_result.get(\"timeout_seconds\", 30),\n",
    "                \"dependencies\": task_result.get(\"dependencies\", []),\n",
    "                \"sql_result\": sql_result,  # Include the complete SQL result data\n",
    "                \"error\": sql_result.get(\"error\") if sql_result else None\n",
    "            },\n",
    "        },\n",
    "        # Add SQL results directly to the payload root for easier access\n",
    "        \"data\": sql_result.get(\"inline\", {}).get(\"rows\", []) if sql_result else [],\n",
    "        \"row_count\": sql_result.get(\"row_count\", 0) if sql_result else 0,\n",
    "        \"columns\": sql_result.get(\"inline\", {}).get(\"columns\", []) if sql_result else [],\n",
    "        \"external_links\": sql_result.get(\"external_links\", []) if sql_result else [],\n",
    "        \"query\": sql_result.get(\"query\", \"\") if sql_result else \"\",\n",
    "        \"sql_execution_time\": sql_result.get(\"execution_time_seconds\", 0) if sql_result else 0,\n",
    "        \"status\": sql_result.get(\"status\", \"UNKNOWN\") if sql_result else \"NO_QUERY\",\n",
    "        \"run_id\": \"\",\n",
    "        \"status\": \"completed\" if sql_result.get(\"status\") == \"SUCCEEDED\" else \"failed\",\n",
    "        \"workflow_run_id\": workflow_run_id,\n",
    "        \"task_key\": task_key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Attempting to send webhook for task_key={task_key}, product_id={product_id}, email={client_email}\")\n",
    "        logger.info(f\"Webhook URL: {webhook_url}\")\n",
    "        logger.debug(f\"Webhook payload: {json.dumps(individual_payload, indent=2)}\")\n",
    "        \n",
    "        # Make actual HTTP request to webhook\n",
    "        import requests\n",
    "        response = requests.post(webhook_url, json=individual_payload, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        logger.info(f\"Individual webhook sent successfully for {task_key}. Status: {response.status_code}\")\n",
    "        return True\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"HTTP error sending individual webhook for {task_key}: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error sending individual webhook for {task_key}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def build_dependency_graph(tasks: List[Dict[str, Any]]) -> Dict[str, List[str]]:\n",
    "    \"\"\"Build a dependency graph from tasks\"\"\"\n",
    "    dependency_graph = {}\n",
    "    \n",
    "    for task in tasks:\n",
    "        task_key = task.get(\"task_key\")\n",
    "        dependencies = [dep.get(\"task_key\") for dep in task.get(\"depends_on\", [])]\n",
    "        dependency_graph[task_key] = dependencies\n",
    "    \n",
    "    return dependency_graph\n",
    "\n",
    "\n",
    "def topological_sort(dependency_graph: Dict[str, List[str]]) -> List[str]:\n",
    "    \"\"\"Perform topological sort to determine execution order\"\"\"\n",
    "    from collections import deque, defaultdict\n",
    "    \n",
    "    # Get all tasks from the dependency graph\n",
    "    all_tasks = set(dependency_graph.keys())\n",
    "    \n",
    "    # Calculate in-degrees (how many dependencies each task has)\n",
    "    in_degree = defaultdict(int)\n",
    "    \n",
    "    # Initialize all tasks with 0 in-degree\n",
    "    for task in all_tasks:\n",
    "        in_degree[task] = 0\n",
    "    \n",
    "    # Count dependencies for each task\n",
    "    for task in dependency_graph:\n",
    "        dependencies = dependency_graph[task]\n",
    "        in_degree[task] = len(dependencies)\n",
    "        \n",
    "        # Ensure all dependency tasks are in our task set\n",
    "        for dep in dependencies:\n",
    "            if dep not in all_tasks:\n",
    "                logger.warning(f\"Dependency {dep} for task {task} not found in task list\")\n",
    "    \n",
    "    # Find tasks with no dependencies (in-degree = 0)\n",
    "    queue = deque([task for task in all_tasks if in_degree[task] == 0])\n",
    "    execution_order = []\n",
    "    \n",
    "    logger.info(f\"Initial queue (no dependencies): {list(queue)}\")\n",
    "    logger.info(f\"In-degrees: {dict(in_degree)}\")\n",
    "    \n",
    "    while queue:\n",
    "        current_task = queue.popleft()\n",
    "        execution_order.append(current_task)\n",
    "        logger.debug(f\"Processing task: {current_task}\")\n",
    "        \n",
    "        # Find tasks that depend on the current task and reduce their in-degree\n",
    "        for task in dependency_graph:\n",
    "            if current_task in dependency_graph[task]:\n",
    "                in_degree[task] -= 1\n",
    "                logger.debug(f\"Reduced in-degree for {task} to {in_degree[task]}\")\n",
    "                if in_degree[task] == 0:\n",
    "                    queue.append(task)\n",
    "                    logger.debug(f\"Added {task} to queue\")\n",
    "    \n",
    "    # Check for circular dependencies\n",
    "    if len(execution_order) != len(all_tasks):\n",
    "        remaining_tasks = all_tasks - set(execution_order)\n",
    "        logger.error(f\"Circular dependency detected! Remaining tasks: {remaining_tasks}\")\n",
    "        logger.error(f\"In-degrees of remaining tasks: {[(task, in_degree[task]) for task in remaining_tasks]}\")\n",
    "        # Add remaining tasks to execution order anyway (they'll fail with dependency errors)\n",
    "        execution_order.extend(remaining_tasks)\n",
    "    \n",
    "    return execution_order\n",
    "\n",
    "\n",
    "def process_workflow(workflow_payload: Dict[str, Any], debug: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"Process the entire workflow with proper dependency resolution\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Extract workflow information\n",
    "    workflow_name = workflow_payload.get(\"name\", \"\")\n",
    "    workflow_tags = workflow_payload.get(\"tags\", {})\n",
    "    workflow_run_id = workflow_tags.get(\"workflow_run_id\", f\"wf_run_{int(time.time())}\")\n",
    "    main_product_id = workflow_tags.get(\"main_product_id\", \"\")\n",
    "    \n",
    "    # Extract client info from email notifications or tags\n",
    "    email_notifications = workflow_payload.get(\"email_notifications\", {})\n",
    "    emails = email_notifications.get(\"on_success\", [\"\"])\n",
    "    client_email = emails[0] if emails else \"\"\n",
    "    client_id = workflow_run_id.split('_')[-1]  # Extract from workflow_run_id\n",
    "    \n",
    "    tasks = workflow_payload.get(\"tasks\", [])\n",
    "    \n",
    "    logger.info(f\"Processing workflow: {workflow_name}\")\n",
    "    logger.info(f\"Workflow Run ID: {workflow_run_id}\")\n",
    "    logger.info(f\"Main Product ID: {main_product_id}\")\n",
    "    logger.info(f\"Total Tasks: {len(tasks)}\")\n",
    "    logger.info(\"NOTE: Using dummy data generation instead of SQL warehouse\")\n",
    "    \n",
    "    workflow_info = {\n",
    "        \"workflow_run_id\": workflow_run_id,\n",
    "        \"client_id\": client_id,\n",
    "        \"email\": client_email,\n",
    "        \"main_product_id\": main_product_id\n",
    "    }\n",
    "    \n",
    "    # Build dependency graph and determine execution order\n",
    "    dependency_graph = build_dependency_graph(tasks)\n",
    "    execution_order = topological_sort(dependency_graph)\n",
    "    \n",
    "    logger.info(f\"Execution order determined: {execution_order}\")\n",
    "    \n",
    "    # Create task lookup\n",
    "    task_lookup = {task.get(\"task_key\"): task for task in tasks}\n",
    "    \n",
    "    # Initialize result structure\n",
    "    workflow_result = {\n",
    "        \"workflow_name\": workflow_name,\n",
    "        \"workflow_run_id\": workflow_run_id,\n",
    "        \"main_product_id\": main_product_id,\n",
    "        \"client_id\": client_id,\n",
    "        \"email\": client_email,\n",
    "        \"execution_summary\": {\n",
    "            \"total_tasks\": len(tasks),\n",
    "            \"successful_tasks\": 0,\n",
    "            \"failed_tasks\": 0,\n",
    "            \"start_time\": datetime.now().isoformat(),\n",
    "            \"end_time\": None,\n",
    "            \"total_execution_time\": 0,\n",
    "            \"webhooks_sent\": []\n",
    "        },\n",
    "        \"task_results\": [],\n",
    "        \"execution_order\": execution_order\n",
    "    }\n",
    "    \n",
    "    # Process tasks in dependency order\n",
    "    completed_tasks = set()\n",
    "    failed_tasks = set()\n",
    "    \n",
    "    try:\n",
    "        for task_key in execution_order:\n",
    "            if task_key not in task_lookup:\n",
    "                logger.warning(f\"Task {task_key} not found in task lookup, skipping\")\n",
    "                continue\n",
    "                \n",
    "            task = task_lookup[task_key]\n",
    "            \n",
    "            # Check if all dependencies are completed successfully\n",
    "            dependencies = dependency_graph.get(task_key, [])\n",
    "            failed_dependencies = []\n",
    "            \n",
    "            for dep_task_key in dependencies:\n",
    "                if dep_task_key in failed_tasks:\n",
    "                    failed_dependencies.append(dep_task_key)\n",
    "                elif dep_task_key not in completed_tasks:\n",
    "                    # This shouldn't happen with proper topological sort, but handle it\n",
    "                    logger.warning(f\"Dependency {dep_task_key} for task {task_key} not yet completed\")\n",
    "                    failed_dependencies.append(dep_task_key)\n",
    "            \n",
    "            if failed_dependencies:\n",
    "                logger.error(f\"Task {task_key} cannot execute - failed dependencies: {failed_dependencies}\")\n",
    "                task_result = {\n",
    "                    \"task_key\": task_key,\n",
    "                    \"product_id\": task.get(\"sql_task\", {}).get(\"parameters\", {}).get(\"product_id\", \"\"),\n",
    "                    \"description\": task.get(\"description\", \"\"),\n",
    "                    \"status\": \"FAILED\",\n",
    "                    \"error\": f\"Dependencies failed: {failed_dependencies}\",\n",
    "                    \"start_time\": datetime.now().isoformat(),\n",
    "                    \"end_time\": datetime.now().isoformat(),\n",
    "                    \"dependencies\": dependencies,\n",
    "                    \"calculation_order\": int(task.get(\"sql_task\", {}).get(\"parameters\", {}).get(\"calculation_order\", \"0\")),\n",
    "                    \"timeout_seconds\": task.get(\"timeout_seconds\", 30),\n",
    "                    \"sql_result\": {\"status\": \"FAILED\", \"error\": f\"Dependencies failed: {failed_dependencies}\"}\n",
    "                }\n",
    "                failed_tasks.add(task_key)\n",
    "                workflow_result[\"execution_summary\"][\"failed_tasks\"] += 1\n",
    "            else:\n",
    "                # Process the task\n",
    "                logger.info(f\"Executing task {task_key} - all dependencies satisfied: {dependencies}\")\n",
    "                task_result = process_task(task, workflow_run_id, debug)\n",
    "                \n",
    "                if task_result.get(\"status\") == \"SUCCEEDED\":\n",
    "                    completed_tasks.add(task_key)\n",
    "                    workflow_result[\"execution_summary\"][\"successful_tasks\"] += 1\n",
    "                    logger.info(f\"Task {task_key} completed successfully with dummy data\")\n",
    "                else:\n",
    "                    failed_tasks.add(task_key)\n",
    "                    workflow_result[\"execution_summary\"][\"failed_tasks\"] += 1\n",
    "                    logger.error(f\"Task {task_key} failed: {task_result.get('error', 'Unknown error')}\")\n",
    "            \n",
    "            workflow_result[\"task_results\"].append(task_result)\n",
    "            \n",
    "            # Send individual webhook for this task with workflow_payload\n",
    "            logger.info(f\"About to send webhook for task {task_key}\")\n",
    "            webhook_sent = webhook_notify_individual_task(task_result, workflow_info, workflow_payload)\n",
    "            workflow_result[\"execution_summary\"][\"webhooks_sent\"].append({\n",
    "                \"task_key\": task_key,\n",
    "                \"product_id\": task_result.get(\"product_id\"),\n",
    "                \"webhook_sent\": webhook_sent,\n",
    "                \"status\": task_result.get(\"status\")\n",
    "            })\n",
    "            logger.info(f\"Webhook result for task {task_key}: {'sent' if webhook_sent else 'failed'}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing workflow: {str(e)}\")\n",
    "        workflow_result[\"error\"] = str(e)\n",
    "    \n",
    "    # Finalize execution summary\n",
    "    total_time = time.time() - start_time\n",
    "    workflow_result[\"execution_summary\"][\"end_time\"] = datetime.now().isoformat()\n",
    "    workflow_result[\"execution_summary\"][\"total_execution_time\"] = total_time\n",
    "    \n",
    "    # Determine overall status\n",
    "    if workflow_result[\"execution_summary\"][\"failed_tasks\"] == 0:\n",
    "        overall_status = \"SUCCESS\"\n",
    "    elif workflow_result[\"execution_summary\"][\"successful_tasks\"] > 0:\n",
    "        overall_status = \"PARTIAL_SUCCESS\"\n",
    "    else:\n",
    "        overall_status = \"FAILED\"\n",
    "    \n",
    "    workflow_result[\"overall_status\"] = overall_status\n",
    "    \n",
    "    logger.info(f\"Workflow processing completed in {total_time:.2f}s with dummy data\")\n",
    "    logger.info(f\"Summary: {workflow_result['execution_summary']['successful_tasks']}/{workflow_result['execution_summary']['total_tasks']} tasks successful\")\n",
    "    \n",
    "    # Return summary for notebook output\n",
    "    successful_webhooks = sum(1 for w in workflow_result[\"execution_summary\"][\"webhooks_sent\"] if w[\"webhook_sent\"])\n",
    "    \n",
    "    return {\n",
    "        \"processing_summary\": {\n",
    "            \"workflow_run_id\": workflow_run_id,\n",
    "            \"workflow_name\": workflow_name,\n",
    "            \"total_tasks_processed\": len(workflow_result[\"task_results\"]),\n",
    "            \"successful_tasks\": workflow_result[\"execution_summary\"][\"successful_tasks\"],\n",
    "            \"failed_tasks\": workflow_result[\"execution_summary\"][\"failed_tasks\"],\n",
    "            \"successful_webhooks\": successful_webhooks,\n",
    "            \"failed_webhooks\": len(workflow_result[\"execution_summary\"][\"webhooks_sent\"]) - successful_webhooks,\n",
    "            \"execution_time\": total_time,\n",
    "            \"status\": overall_status,\n",
    "            \"dummy_data_mode\": True\n",
    "        },\n",
    "        \"individual_webhooks_sent\": workflow_result[\"execution_summary\"][\"webhooks_sent\"],\n",
    "        \"execution_order\": execution_order\n",
    "    }\n",
    "\n",
    "\n",
    "# Databricks notebook entry point\n",
    "try:\n",
    "    payload_str = dbutils.widgets.get(\"payload\")\n",
    "    logger.info(f\"Raw payload string length: {len(payload_str) if payload_str else 0}\")\n",
    "    logger.info(f\"Raw payload preview (first 200 chars): {payload_str[:200] if payload_str else 'None'}\")\n",
    "    \n",
    "    # Enhanced JSON parsing with better error handling\n",
    "    if not payload_str or payload_str.strip() in [\"{}\", \"\"]:\n",
    "        raise ValueError(\"Empty or default payload received\")\n",
    "    \n",
    "    try:\n",
    "        payload = json.loads(payload_str)\n",
    "    except json.JSONDecodeError as je:\n",
    "        logger.error(f\"JSON decode error at position {je.pos}: {je.msg}\")\n",
    "        logger.error(f\"Problem area: '{payload_str[max(0, je.pos-50):je.pos+50]}'\")\n",
    "        \n",
    "        # Try to fix common JSON issues\n",
    "        cleaned_payload = payload_str.strip()\n",
    "        \n",
    "        # Remove any trailing commas before closing brackets/braces\n",
    "        import re\n",
    "        cleaned_payload = re.sub(r',(\\s*[}\\]])', r'\\1', cleaned_payload)\n",
    "        \n",
    "        # Try parsing the cleaned version\n",
    "        try:\n",
    "            payload = json.loads(cleaned_payload)\n",
    "            logger.info(\"Successfully parsed JSON after cleaning\")\n",
    "        except json.JSONDecodeError as je2:\n",
    "            logger.error(f\"Still failed after cleaning at position {je2.pos}: {je2.msg}\")\n",
    "            logger.error(f\"Full payload causing error: {payload_str}\")\n",
    "            raise ValueError(f\"Invalid JSON payload: {je2.msg} at position {je2.pos}\")\n",
    "\n",
    "    # Check if this is the new workflow format\n",
    "    if \"tasks\" in payload and \"name\" in payload:\n",
    "        logger.info(\"Detected new workflow format payload - Running in DUMMY DATA mode\")\n",
    "        debug = payload.get(\"debug\", False)\n",
    "        \n",
    "        result = process_workflow(\n",
    "            workflow_payload=payload,\n",
    "            debug=debug\n",
    "        )\n",
    "        \n",
    "        # The result contains a processing summary for the workflow\n",
    "        final_result = {\n",
    "            \"status\": \"SUCCESS\" if result[\"processing_summary\"][\"status\"] not in [\"FAILED\", \"ERROR\"] else \"ERROR\",\n",
    "            \"message\": f\"Processed workflow with {result['processing_summary']['total_tasks_processed']} tasks using DUMMY DATA\",\n",
    "            \"processing_summary\": result[\"processing_summary\"],\n",
    "            \"individual_webhooks\": result[\"individual_webhooks_sent\"],\n",
    "            \"execution_order\": result[\"execution_order\"],\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"dummy_data_mode\": True\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        # Legacy format - keep existing formula processing logic\n",
    "        logger.info(\"Detected legacy formula format payload\")\n",
    "        raise ValueError(\"Legacy formula format is no longer supported. Please use the new workflow format.\")\n",
    "\n",
    "    print(\"=== WORKFLOW PROCESSING RESULT (DUMMY DATA MODE) ===\")\n",
    "    print(json.dumps(final_result, indent=2, default=str))\n",
    "\n",
    "    # Simplified result for notebook exit\n",
    "    simple_result = {\n",
    "        \"status\": final_result[\"status\"],\n",
    "        \"workflow_run_id\": result[\"processing_summary\"][\"workflow_run_id\"],\n",
    "        \"workflow_name\": result[\"processing_summary\"][\"workflow_name\"],\n",
    "        \"total_tasks_processed\": result[\"processing_summary\"][\"total_tasks_processed\"],\n",
    "        \"successful_tasks\": result[\"processing_summary\"][\"successful_tasks\"],\n",
    "        \"failed_tasks\": result[\"processing_summary\"][\"failed_tasks\"],\n",
    "        \"successful_webhooks\": result[\"processing_summary\"][\"successful_webhooks\"],\n",
    "        \"failed_webhooks\": result[\"processing_summary\"][\"failed_webhooks\"],\n",
    "        \"execution_time\": result[\"processing_summary\"][\"execution_time\"],\n",
    "        \"message\": \"Workflow tasks processed with dummy data and individual webhooks sent\",\n",
    "        \"dummy_data_mode\": True\n",
    "    }\n",
    "\n",
    "    # dbutils.notebook.exit(json.dumps(simple_result, default=str))\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error processing payload: {str(e)}\")\n",
    "    error_result = {\n",
    "        \"status\": \"FAILED\",\n",
    "        \"error\": str(e),\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"dummy_data_mode\": True\n",
    "    }\n",
    "\n",
    "    print(\"=== ERROR RESULT ===\")\n",
    "    print(json.dumps(error_result, indent=2))\n",
    "    \n",
    "    # dbutils.notebook.exit(json.dumps(error_result))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
